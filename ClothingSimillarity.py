# -*- coding: utf-8 -*-
"""Untitled78.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1creUAec6zVQUCWSNZ0arMXvyuGx58NQf
"""

import nltk
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from flask import Flask, jsonify, request
from google.cloud import storage

# Download NLTK corpora from Google Cloud Storage
bucket_name = "clothingsimillarity"
nltk_files = ["nltk_data/corpora/stopwords", "nltk_data/corpora/wordnet"]

storage_client = storage.Client()
bucket = storage_client.get_bucket(bucket_name)

for file in nltk_files:
    blob = storage.blob.Blob(file, bucket)
    blob.download_to_filename(file)

nltk.data.path.append("nltk_data")

# Initialize a WordNet lemmatizer
lemmatizer = WordNetLemmatizer()
def read_csv_gcs(bucket_name, file_path):
    client = storage.Client()
    bucket = client.get_bucket(bucket_name)
    blob = storage.Blob(file_path, bucket)
    content = blob.download_as_text()
    df = pd.read_csv(BytesIO(content))
    return df
# Clean the item description
def clean_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])
    
    # Lemmatization
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])

    return text
# Load the data
df_asos = read_csv_gcs('clothingsimillarity', 'ASOS (1).csv')
df_flipkart = read_csv_gcs('clothingsimillarity', 'flip_kart.csv')
df_zalando = read_csv_gcs('clothingsimillarity', 'zalando.csv')
df_macys = read_csv_gcs('clothingsimillarity', "Macy's.csv")
# Rename the columns for consistency
df_asos.rename(columns = {'description':'item_description','url':'url'}, inplace = True)
df_flipkart.rename(columns = {'title':'item_description','link':'url'}, inplace = True)
df_zalando.rename(columns = {'Description':'item_description','Link':'url'}, inplace = True)
df_macys.rename(columns = {'Description':'item_description','Link':'url'}, inplace = True)

# Clean the item descriptions
for df in [df_asos, df_flipkart, df_zalando, df_macys]:
    df.dropna(inplace=True)
    df.drop_duplicates(inplace=True)
    df['item_description'] = df['item_description'].apply(clean_text)

# Combine all dataframes into one
df_all = pd.concat([df_asos, df_flipkart, df_zalando, df_macys])

# Create a sentence transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Train the model with your data
embeddings = model.encode(df_all['item_description'].tolist())

# Add the embeddings to the dataframe
df_all['embedding'] = embeddings.tolist()

# Create a Flask app
app = Flask(__name__)

@app.route('/find-similar-items', methods=['POST'])
def find_similar_items():
    data = request.get_json()
    
    item_description = data['item_description']
    
    if not isinstance(item_description, str):
        raise ValueError("Item description must be a string.")
    
    item_embedding = model.encode([item_description])[0]
    
    # Calculate the cosine similarity with all other items
    similarities = cosine_similarity([item_embedding], embeddings)
    
    # Get the top 5 most similar items indices
    top_similar_items_indices = similarities[0].argsort()[-5:][::-1]
    
    # Fetch the similar items details
    similar_items = df_all.iloc[top_similar_items_indices]
    
    return jsonify(similar_items.to_dict(orient='records'))

if __name__ == '__main__':
    app.run()