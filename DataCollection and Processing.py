# -*- coding: utf-8 -*-
"""Mercor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TK8_ndOxGuakl9ju9OIj-9JrZKBt5Fl5

# ASOS
"""

import requests
import pandas as pd
from bs4 import BeautifulSoup


def get_data(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup


def parse(soup):
    product_list = []

    for article in soup.find_all('article'): 
        product = {}
        product_link = article.find('a', {'class': 'productLink_KM4PI'})
        product_description_div = article.find('div', {'class': 'productDescription_sryaw'})

        if product_link is not None:
            product['url'] = product_link.get('href')
        else:
            product['url'] = None

        if product_description_div is not None:
            description_p = product_description_div.find('p')
            if description_p is not None:
                product['description'] = description_p.text
            else:
                product['description'] = None
        else:
            product['description'] = None

        product_list.append(product)

    # print(f"Scraped {len(product_list)} products.")
    return product_list


def output(results):
    df = pd.DataFrame(results, columns=['description', 'url'])
    df.to_csv('ASOS.csv', index=False)


def main():
    base_url = "https://www.asos.com/men/shirts/cat/?ew6325sf1=fail&cid=3602&page="
    pages = 50  # adjust to the actual number of pages
    results = []

    for page in range(1, pages + 1):
        url = base_url + str(page)
        soup = get_data(url)
        page_results = parse(soup)
        results.extend(page_results)

    output(results)


if __name__ == '__main__':
    main()

"""# Flip kart"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

def get_data(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"}
    response = requests.get(url, headers=headers)
    return response.text

def parse(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    product_list = []
    
    products = soup.find_all('div', {'class': '_1xHGtK _373qXS'})
    for product in products:
        product_info = {}

        link = product.find('a', {'class': '_2UzuFa'})
        if link is not None:
            product_info['link'] = "https://www.flipkart.com" + link.get('href')
        else:
            product_info['link'] = None

        title = product.find('a', {'class': 'IRpwTa'})
        if title is not None:
            product_info['title'] = title.get('title')
            
        else:
            product_info['title'] = None

        product_list.append(product_info)
    return product_list

base_url = "https://www.flipkart.com/clothing-and-accessories/topwear/pr?sid=clo,ash&p[]=facets.ideal_for%255B%255D%3DMen&p[]=facets.ideal_for%255B%255D%3Dmen&otracker=categorytree&fm=neo%2Fmerchandising&iid=M_be93dda2-6546-4217-a977-b64db98da442_1_372UD5BXDFYS_MC.6XNZG1FYFBZT&otracker=hp_rich_navigation_1_1.navigationCard.RICH_NAVIGATION_Fashion~Men%2527s%2BTop%2BWear_6XNZG1FYFBZT&otracker1=hp_rich_navigation_PINNED_neo%2Fmerchandising_NA_NAV_EXPANDABLE_navigationCard_cc_1_L1_view-all&cid=6XNZG1FYFBZT&page="

data = []
for page in range(1, 200):  # Loop through all 545 pages
    url = base_url + str(page)
    html_content = get_data(url)
    products = parse(html_content)
    data.extend(products)

df = pd.DataFrame(data)
df.to_csv('flip_kart.csv', index=False)

"""


# Zalando


"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

def get_soup(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")
    return soup

def extract_links_and_descriptions(soup):
    parent_divs = soup.find_all("div", class_="L5YdXz _0xLoFW _7ckuOK mROyo1", attrs={"data-zalon-partner-target": "true"})
    data = []
    for parent_div in parent_divs:
        child_divs = parent_div.find_all("div", class_="_5qdMrS w8MdNG cYylcv BaerYO _75qWlu iOzucJ JT3_zV _Qe9k6")
        for child_div in child_divs:
            article_tag = child_div.find("article", class_="z5x6ht _0xLoFW JT3_zV mo6ZnF _78xIQ-")
            if article_tag:
                link_tag = article_tag.find('a',class_="_LM JT3_zV CKDt_l CKDt_l LyRfpJ")
                if link_tag and not link_tag['href'].startswith("/outfits"):
                    link = link_tag['href']
                    desc_div = article_tag.find('div', class_="_0xLoFW _78xIQ- EJ4MLB f4ql6o JT3_zV")
                    desc_link = desc_div.find('a', class_="q84f1m CKDt_l LyRfpJ JT3_zV CKDt_l _2dqvZS")
                    if desc_link:
                        description_tag = desc_link.find('h3', class_="KxHAYs lystZ1 FxZV-M _4F506m ZkIJC- r9BRio qXofat EKabf7 nBq1-s _2MyPg2")
                        if description_tag:
                            description = description_tag.text
                            data.append({"Link": link, "Description": description})
    return data

def scrape_pages(start_page, end_page):
    all_data = []
    for page in range(start_page, end_page+1):
        print(f"Scraping page {page}")
        url = f"https://www.zalando.co.uk/mens-clothing-shirts/?p={page}"
        soup = get_soup(url)
        page_data = extract_links_and_descriptions(soup)
        all_data.extend(page_data)
    return all_data

def main():
    start_page = 1
    end_page = 56
    data = scrape_pages(start_page, end_page)
    df = pd.DataFrame(data)
    df.to_csv('zalando.csv',index=False)

if __name__ == "__main__":
    main()

"""# Macy"""

def get_soup(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")
    return soup

def extract_links_and_descriptions_macys(soup):
    product_items = soup.find_all("li", class_="cell productThumbnailItem")
    data = []
    for product in product_items:
        link_tag = product.find("a", class_="productDescLink productDescLinkEllipsis")
        if link_tag:
            link =  'https://www.macys.com'+link_tag['href']
            description = link_tag['title']
            data.append({"Link": link, "Description": description})
    return data

def scrape_pages_macys(start_page, end_page):
    all_data = []
    for page in range(start_page, end_page+1):
        print(f"Scraping page {page}")
        url = f"https://www.macys.com/shop/mens-clothing/mens-shirts/Pageindex/{page}?id=20626"
        soup = get_soup(url)
        page_data = extract_links_and_descriptions_macys(soup)
        all_data.extend(page_data)
    return all_data

def main_macys():
    start_page = 1
    end_page = 56
    data = scrape_pages_macys(start_page, end_page)
    df = pd.DataFrame(data)
    return df

data=main_macys()

data.to_csv("Macy's.csv",index=False)